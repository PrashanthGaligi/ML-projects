{"cells":[{"cell_type":"markdown","id":"c893d81f-57b5-43a2-8b50-2154c6dcfb4e","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"source":["# Using XGBoost in Python Tutorial \n","\n","Discover the power of XGBoost, one of the most popular machine learning frameworks among data scientists, with this step-by-step tutorial in Python."]},{"cell_type":"markdown","id":"45706eb0-21cb-4989-b755-1207c3ce5078","metadata":{},"source":["XGBoost is one of the most popular machine learning frameworks among data scientists. According to the Kaggle [State of Data Science Survey 2021](https://www.kaggle.com/kaggle-survey-2021), almost 50% of respondents said they used XGBoost, ranking below only TensorFlow and Sklearn. \n","\n","\n","\n","https://www.kaggle.com/kaggle-survey-2021\n","\n","\n","\n","This XGBoost tutorial will introduce the key aspects of this popular Python framework, exploring how you can use it for your own machine learning projects.\n","\n","## What You Will Learn in This Python XGBoost Tutorial\n","\n","Throughout this tutorial, we will cover the key aspects of XGBoost, including:\n","\n","- Installation\n","- XGBoost DMatrix class\n","- XGBoost regression\n","- Objective and loss functions in XGBoost\n","- Building training and evaluation loops\n","- Cross-validation in XGBoost\n","- Building an XGBoost classifier\n","- Changing between Sklearn and native APIs of XGBoost\n","\n","\n","Let’s get started!"]},{"cell_type":"markdown","id":"a5cc777c-14e0-4219-8fa5-4cb51bf26720","metadata":{},"source":["## Loading and Exploring the Data\n","\n","We will be working with the Diamonds dataset throughout the tutorial. It is built into the Seaborn library, or alternatively, you can also [download it from Kaggle](https://www.kaggle.com/datasets/shivam2503/diamonds). It has a nice combination of numeric and categorical features and over 50k observations that we can comfortably showcase all the advantages of XGBoost."]},{"cell_type":"code","execution_count":63,"id":"697f5e24-8c4e-4e1c-a8ed-7a2498e2d12b","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>carat</th>\n","      <th>cut</th>\n","      <th>color</th>\n","      <th>clarity</th>\n","      <th>depth</th>\n","      <th>table</th>\n","      <th>price</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>z</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.23</td>\n","      <td>Ideal</td>\n","      <td>E</td>\n","      <td>SI2</td>\n","      <td>61.5</td>\n","      <td>55.0</td>\n","      <td>326</td>\n","      <td>3.95</td>\n","      <td>3.98</td>\n","      <td>2.43</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.21</td>\n","      <td>Premium</td>\n","      <td>E</td>\n","      <td>SI1</td>\n","      <td>59.8</td>\n","      <td>61.0</td>\n","      <td>326</td>\n","      <td>3.89</td>\n","      <td>3.84</td>\n","      <td>2.31</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.23</td>\n","      <td>Good</td>\n","      <td>E</td>\n","      <td>VS1</td>\n","      <td>56.9</td>\n","      <td>65.0</td>\n","      <td>327</td>\n","      <td>4.05</td>\n","      <td>4.07</td>\n","      <td>2.31</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.29</td>\n","      <td>Premium</td>\n","      <td>I</td>\n","      <td>VS2</td>\n","      <td>62.4</td>\n","      <td>58.0</td>\n","      <td>334</td>\n","      <td>4.20</td>\n","      <td>4.23</td>\n","      <td>2.63</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.31</td>\n","      <td>Good</td>\n","      <td>J</td>\n","      <td>SI2</td>\n","      <td>63.3</td>\n","      <td>58.0</td>\n","      <td>335</td>\n","      <td>4.34</td>\n","      <td>4.35</td>\n","      <td>2.75</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   carat      cut color clarity  depth  table  price     x     y     z\n","0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n","1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n","2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n","3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n","4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import warnings\n","import xgboost\n","\n","warnings.filterwarnings(\"ignore\")\n","diamonds = sns.load_dataset(\"diamonds\")\n","diamonds.head()\n"]},{"cell_type":"code","execution_count":34,"id":"1c023821-ad93-44a6-a4e5-997a6c3da7c6","metadata":{},"outputs":[{"data":{"text/plain":["(53940, 10)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["diamonds.shape"]},{"cell_type":"markdown","id":"8766bb0b-2365-425d-8d35-3632763f5ffc","metadata":{},"source":["In a typical real-world project, you would want to spend a lot more time exploring the dataset and visualizing its features. But since this data comes built-in to Seaborn, it is relatively clean.\n","\n","So, we will just look at the 5-number summary of the numeric and categorical features and get going. You can spend a few moments to familiarize yourself with the dataset."]},{"cell_type":"code","execution_count":35,"id":"b994ba2e-d1b7-42ea-971b-99bbcc1ba7d8","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>carat</th>\n","      <th>depth</th>\n","      <th>table</th>\n","      <th>price</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>z</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>53940.000000</td>\n","      <td>53940.000000</td>\n","      <td>53940.000000</td>\n","      <td>53940.000000</td>\n","      <td>53940.000000</td>\n","      <td>53940.000000</td>\n","      <td>53940.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.797940</td>\n","      <td>61.749405</td>\n","      <td>57.457184</td>\n","      <td>3932.799722</td>\n","      <td>5.731157</td>\n","      <td>5.734526</td>\n","      <td>3.538734</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.474011</td>\n","      <td>1.432621</td>\n","      <td>2.234491</td>\n","      <td>3989.439738</td>\n","      <td>1.121761</td>\n","      <td>1.142135</td>\n","      <td>0.705699</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.200000</td>\n","      <td>43.000000</td>\n","      <td>43.000000</td>\n","      <td>326.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.400000</td>\n","      <td>61.000000</td>\n","      <td>56.000000</td>\n","      <td>950.000000</td>\n","      <td>4.710000</td>\n","      <td>4.720000</td>\n","      <td>2.910000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.700000</td>\n","      <td>61.800000</td>\n","      <td>57.000000</td>\n","      <td>2401.000000</td>\n","      <td>5.700000</td>\n","      <td>5.710000</td>\n","      <td>3.530000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.040000</td>\n","      <td>62.500000</td>\n","      <td>59.000000</td>\n","      <td>5324.250000</td>\n","      <td>6.540000</td>\n","      <td>6.540000</td>\n","      <td>4.040000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>5.010000</td>\n","      <td>79.000000</td>\n","      <td>95.000000</td>\n","      <td>18823.000000</td>\n","      <td>10.740000</td>\n","      <td>58.900000</td>\n","      <td>31.800000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              carat         depth         table         price             x  \\\n","count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n","mean       0.797940     61.749405     57.457184   3932.799722      5.731157   \n","std        0.474011      1.432621      2.234491   3989.439738      1.121761   \n","min        0.200000     43.000000     43.000000    326.000000      0.000000   \n","25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n","50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n","75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \n","max        5.010000     79.000000     95.000000  18823.000000     10.740000   \n","\n","                  y             z  \n","count  53940.000000  53940.000000  \n","mean       5.734526      3.538734  \n","std        1.142135      0.705699  \n","min        0.000000      0.000000  \n","25%        4.720000      2.910000  \n","50%        5.710000      3.530000  \n","75%        6.540000      4.040000  \n","max       58.900000     31.800000  "]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["diamonds.describe()"]},{"cell_type":"code","execution_count":36,"id":"ba8f9655-0378-4d6d-a4cb-03b0887aa350","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cut</th>\n","      <th>color</th>\n","      <th>clarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>53940</td>\n","      <td>53940</td>\n","      <td>53940</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>Ideal</td>\n","      <td>G</td>\n","      <td>SI1</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>21551</td>\n","      <td>11292</td>\n","      <td>13065</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          cut  color clarity\n","count   53940  53940   53940\n","unique      5      7       8\n","top     Ideal      G     SI1\n","freq    21551  11292   13065"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["diamonds.describe(exclude=np.number)"]},{"cell_type":"markdown","id":"85b47ea7-6a2d-4b19-abb7-8beb9556d9e8","metadata":{},"source":["## How to Build an XGBoost DMatrix\n","\n","After you are done with exploration, the first step in any project is framing the machine learning problem and extracting the feature and target arrays based on the dataset.\n","\n","In this tutorial, we will first try to predict diamond prices using their physical measurements, so our target will be the price column.\n","\n","So, we are isolating the features into X and the target into y:"]},{"cell_type":"code","execution_count":37,"id":"0553fabb-32c0-4e73-8457-1c6b66af9f96","metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Extract feature and target arrays\n","X, y = diamonds.drop('price', axis=1), diamonds[['price']]"]},{"cell_type":"markdown","id":"8ca4903b-222b-4811-ac0f-0be0913fa9ba","metadata":{},"source":["The dataset has three categorical columns. Normally, you would encode them with ordinal or one-hot encoding, but XGBoost has the ability to internally deal with categoricals.\n","\n","The way to enable this feature is to cast the categorical columns into Pandas `category` data type (by default, they are treated as text columns):"]},{"cell_type":"code","execution_count":38,"id":"2a041699","metadata":{},"outputs":[{"data":{"text/plain":["carat       float64\n","cut        category\n","color      category\n","clarity    category\n","depth       float64\n","table       float64\n","x           float64\n","y           float64\n","z           float64\n","dtype: object"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["X.dtypes"]},{"cell_type":"code","execution_count":39,"id":"879a00bc-75c6-4c52-ae53-1cd8a4774e2d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cut\n","color\n","clarity\n"]}],"source":["# Extract text features\n","cats = X.select_dtypes(exclude=np.number).columns.tolist()\n","\n","# Convert to Pandas category\n","for col in cats:\n","   print(col)\n","   X[col] = X[col].astype('category')"]},{"cell_type":"markdown","id":"051c9124-eac6-440c-b4c7-ce9ba85cdd5a","metadata":{},"source":["Now, when you print the `dtypes` attribute, you'll see that we have three `category` features:"]},{"cell_type":"code","execution_count":40,"id":"57a3022f-4522-4f26-9533-657df5ccf8e8","metadata":{},"outputs":[{"data":{"text/plain":["carat       float64\n","cut        category\n","color      category\n","clarity    category\n","depth       float64\n","table       float64\n","x           float64\n","y           float64\n","z           float64\n","dtype: object"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["X.dtypes"]},{"cell_type":"markdown","id":"b8e524cf-12e6-42e4-9749-dc1ed1db20ba","metadata":{},"source":["Let’s split the data into train, and test sets (0.25 test size):"]},{"cell_type":"code","execution_count":41,"id":"84f3aaab-994b-436f-9ede-ef2ec43f3824","metadata":{},"outputs":[],"source":["# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"]},{"cell_type":"markdown","id":"00374769-7a7d-407d-979b-6bc78bb8b4ab","metadata":{},"source":["Now, the important part: XGBoost comes with its own class for storing datasets called DMatrix. It is a highly optimized class for memory and speed. That's why converting datasets into this format is a requirement for the native XGBoost API:"]},{"cell_type":"code","execution_count":42,"id":"02691e29-7fbd-40c4-ba6f-968bb846cd02","metadata":{},"outputs":[],"source":["import xgboost as xgb\n","\n","# Create regression matrices\n","dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n","dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)"]},{"cell_type":"markdown","id":"c1ebcda0-8406-4cb7-8691-a7c7d3cb30ca","metadata":{},"source":["The class accepts both the training features and the labels. To enable automatic encoding of Pandas category columns, we also set `enable_categorical` to True.\n","\n","**Note**:\n","\n","Why are we going with the native API of XGBoost, rather than its Scikit-learn API? While it might be more comfortable to use the Sklearn API at first, later, you’ll realize that the native API of XGBoost contains some excellent features that the former doesn’t support. So, better get used to it from the beginning. However, there is a section at the end where we show how to switch between APIs in a single line of code even after you have trained models.\n","\n","## Python XGBoost Regression\n","\n","After building the DMatrices, you should choose a value for the `objective` parameter. It tells XGBoost the machine learning problem you are trying to solve and what metrics or loss functions to use to solve that problem.\n","\n","For example, to predict diamond prices, which is a regression problem, you can use the common `reg:squarederror` objective. Usually, the name of the objective also contains the name of the loss function for the problem. For regression, it is common to use Root Mean Squared Error, which minimizes the square root of the squared sum of the differences between actual and predicted values. Here is how the metric would look like when implemented in NumPy:\n","\n","```python\n","import numpy as np\n","\n","mse = np.mean((actual - predicted) ** 2)\n","rmse = np.sqrt(mse)\n","```\n","\n","We’ll learn classification objectives later in the tutorial.\n","\n","A note on the difference between a loss function and a performance metric: A loss function is used by machine learning models to minimize the _differences_ between the actual (ground truth) values and model predictions. On the other hand, a **metric** (or metrics) is chosen by the machine learning engineer to measure the _similarity_ between ground truth and model predictions.\n","\n","In short, a loss function should be minimized while a metric should be maximized. A loss function is used during training to guide the model on where to improve. A metric is used during evaluation to measure overall performance.\n","\n","### Training\n","\n","The chosen objective function and any other hyperparameters of XGBoost should be specified in a dictionary, which by convention should be called params:"]},{"cell_type":"code","execution_count":44,"id":"d9e14817-1b13-4d96-93c9-e8154b8f3101","metadata":{},"outputs":[],"source":["# Define hyperparameters\n","params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\n","\n","# In case you have access to a GPU\n","# params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}"]},{"cell_type":"markdown","id":"cafd764b-9dec-4723-871f-ba7ff54ebb87","metadata":{},"source":["Now, we set another parameter called `num_boost_round`, which stands for _number_ _of boosting rounds_. Internally, XGBoost minimizes the loss function RMSE in small incremental rounds (more on this later). This parameter specifies the amount of those rounds.\n","\n","The ideal number of rounds is found through hyperparameter tuning. For now, we will just set it to 100:"]},{"cell_type":"code","execution_count":45,"id":"d559836a-bc4d-486c-b655-613652d3a473","metadata":{},"outputs":[],"source":["n = 100\n","model = xgb.train(\n","   params=params,\n","   dtrain=dtrain_reg,\n","   num_boost_round=n,\n",")"]},{"cell_type":"markdown","id":"3ca4535d-b0bd-4605-bad4-dd010ad300d0","metadata":{},"source":["When XGBoost runs on a GPU, it is blazing fast. If you didn’t receive any errors from the above code, the training was successful!\n","\n","### Evaluation\n","\n","During the boosting rounds, the model object has learned all the patterns of the training set it possibly can. Now, we must measure its performance by testing it on unseen data. That's where our `dtest_reg` DMatrix comes into play:"]},{"cell_type":"code","execution_count":46,"id":"5133e0d5-0bba-4b3a-98ec-1c216d35d8a1","metadata":{},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","\n","preds = model.predict(dtest_reg)"]},{"cell_type":"markdown","id":"421c4fcb-113e-420b-9136-6030d33e5fc6","metadata":{},"source":["This step of the process is called model evaluation (or inference). Once you generate predictions with predict, you pass them inside `mean_squared_error` function of Sklearn to compare against `y_test`:"]},{"cell_type":"code","execution_count":47,"id":"4119ac45-fcb7-4e21-aafa-2f86277e8189","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RMSE of the base model: 545.388\n"]}],"source":["rmse = mean_squared_error(y_test, preds, squared=False)\n","\n","print(f\"RMSE of the base model: {rmse:.3f}\")"]},{"cell_type":"markdown","id":"629f9658-1523-4a27-954f-971d89ff9fd1","metadata":{},"source":["We’ve got a base score ~543$, which was the performance of a base model with default parameters. There are two ways we can improve it— by performing cross-validation and hyperparameter tuning. But before that, let’s see a quicker way of evaluating XGBoost models.\n","\n","## Using Validation Sets During Training\n","\n","Training a machine learning model is like launching a rocket into space. You can control everything about the model up to the launch, but once it does, all you can do is stand by and wait for it to finish.\n","\n","But the problem with our current training process is that we can’t even watch where the model is going. To solve this, we will use evaluation arrays that allow us to see model performance as it gets improved incrementally across boosting rounds.\n","\n","First, let’s set up the parameters again:"]},{"cell_type":"code","execution_count":48,"id":"7d187a35-c045-4aa0-bbb6-3bb77584a63d","metadata":{},"outputs":[],"source":["params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\n","n = 100"]},{"cell_type":"markdown","id":"3ddfbc6d-bf88-41bb-8a0e-98c9c6b89471","metadata":{},"source":["Next, we create a list of two tuples that each contain two elements. The first element is the array for the model to evaluate, and the second is the array’s name."]},{"cell_type":"code","execution_count":49,"id":"b632539c-d982-46e3-a6a0-dc5aea07c149","metadata":{},"outputs":[],"source":["evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]"]},{"cell_type":"markdown","id":"b658cfa6-e3c4-47a9-ab31-d9af29069902","metadata":{},"source":["When we pass this array to the `evals` parameter of `xgb.train`, we will see the model performance after each boosting round:"]},{"cell_type":"code","execution_count":50,"id":"a958e2ea-934a-41c3-84f2-925aad951080","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0]\ttrain-rmse:3985.31595\tvalidation-rmse:3930.87087\n","[1]\ttrain-rmse:2849.92126\tvalidation-rmse:2812.52945\n","[2]\ttrain-rmse:2061.76472\tvalidation-rmse:2034.91266\n","[3]\ttrain-rmse:1521.58802\tvalidation-rmse:1509.03801\n","[4]\ttrain-rmse:1158.20689\tvalidation-rmse:1155.77477\n","[5]\ttrain-rmse:918.95666\tvalidation-rmse:922.81058\n","[6]\ttrain-rmse:765.71970\tvalidation-rmse:778.96367\n","[7]\ttrain-rmse:671.73734\tvalidation-rmse:692.56259\n","[8]\ttrain-rmse:612.92636\tvalidation-rmse:638.83852\n","[9]\ttrain-rmse:578.33182\tvalidation-rmse:608.53984\n","[10]\ttrain-rmse:557.19710\tvalidation-rmse:591.03042\n","[11]\ttrain-rmse:542.58916\tvalidation-rmse:578.99646\n","[12]\ttrain-rmse:534.88302\tvalidation-rmse:573.25964\n","[13]\ttrain-rmse:527.20423\tvalidation-rmse:566.28647\n","[14]\ttrain-rmse:520.90582\tvalidation-rmse:561.97563\n","[15]\ttrain-rmse:515.69808\tvalidation-rmse:558.92935\n","[16]\ttrain-rmse:512.45290\tvalidation-rmse:557.84167\n","[17]\ttrain-rmse:507.50759\tvalidation-rmse:556.68519\n","[18]\ttrain-rmse:504.04144\tvalidation-rmse:553.56230\n","[19]\ttrain-rmse:499.18646\tvalidation-rmse:551.09108\n","[20]\ttrain-rmse:495.31647\tvalidation-rmse:550.76666\n","[21]\ttrain-rmse:491.79040\tvalidation-rmse:550.60082\n","[22]\ttrain-rmse:487.77269\tvalidation-rmse:548.36432\n","[23]\ttrain-rmse:486.55224\tvalidation-rmse:547.99883\n","[24]\ttrain-rmse:485.06522\tvalidation-rmse:548.32492\n","[25]\ttrain-rmse:481.40234\tvalidation-rmse:546.35318\n","[26]\ttrain-rmse:475.79729\tvalidation-rmse:546.48934\n","[27]\ttrain-rmse:474.35138\tvalidation-rmse:545.81126\n","[28]\ttrain-rmse:471.78810\tvalidation-rmse:546.34150\n","[29]\ttrain-rmse:469.57226\tvalidation-rmse:545.78579\n","[30]\ttrain-rmse:467.13670\tvalidation-rmse:547.16647\n","[31]\ttrain-rmse:465.60822\tvalidation-rmse:546.68017\n","[32]\ttrain-rmse:463.18297\tvalidation-rmse:546.35900\n","[33]\ttrain-rmse:461.03645\tvalidation-rmse:546.92292\n","[34]\ttrain-rmse:459.11065\tvalidation-rmse:545.65415\n","[35]\ttrain-rmse:455.58103\tvalidation-rmse:545.60024\n","[36]\ttrain-rmse:453.61231\tvalidation-rmse:545.11648\n","[37]\ttrain-rmse:451.70785\tvalidation-rmse:544.35182\n","[38]\ttrain-rmse:450.80103\tvalidation-rmse:543.49648\n","[39]\ttrain-rmse:447.46143\tvalidation-rmse:544.23386\n","[40]\ttrain-rmse:447.26879\tvalidation-rmse:544.10422\n","[41]\ttrain-rmse:445.31017\tvalidation-rmse:544.32107\n","[42]\ttrain-rmse:443.57201\tvalidation-rmse:544.19011\n","[43]\ttrain-rmse:441.65878\tvalidation-rmse:544.16246\n","[44]\ttrain-rmse:440.49680\tvalidation-rmse:544.10014\n","[45]\ttrain-rmse:439.46627\tvalidation-rmse:543.94863\n","[46]\ttrain-rmse:437.09814\tvalidation-rmse:543.69954\n","[47]\ttrain-rmse:435.52668\tvalidation-rmse:543.50496\n","[48]\ttrain-rmse:433.53532\tvalidation-rmse:544.29780\n","[49]\ttrain-rmse:432.63530\tvalidation-rmse:544.03805\n","[50]\ttrain-rmse:432.51681\tvalidation-rmse:543.97371\n","[51]\ttrain-rmse:432.34761\tvalidation-rmse:543.84972\n","[52]\ttrain-rmse:432.29726\tvalidation-rmse:543.74148\n","[53]\ttrain-rmse:429.82478\tvalidation-rmse:544.32619\n","[54]\ttrain-rmse:428.05758\tvalidation-rmse:544.44648\n","[55]\ttrain-rmse:427.29817\tvalidation-rmse:544.98653\n","[56]\ttrain-rmse:425.98187\tvalidation-rmse:545.03108\n","[57]\ttrain-rmse:423.33404\tvalidation-rmse:545.05312\n","[58]\ttrain-rmse:422.41177\tvalidation-rmse:545.31690\n","[59]\ttrain-rmse:421.42258\tvalidation-rmse:545.08016\n","[60]\ttrain-rmse:420.72943\tvalidation-rmse:544.77874\n","[61]\ttrain-rmse:420.07456\tvalidation-rmse:544.64864\n","[62]\ttrain-rmse:418.52485\tvalidation-rmse:543.86081\n","[63]\ttrain-rmse:416.56287\tvalidation-rmse:544.48380\n","[64]\ttrain-rmse:415.38839\tvalidation-rmse:544.66277\n","[65]\ttrain-rmse:413.79059\tvalidation-rmse:544.35353\n","[66]\ttrain-rmse:412.29481\tvalidation-rmse:545.02371\n","[67]\ttrain-rmse:411.09605\tvalidation-rmse:545.16844\n","[68]\ttrain-rmse:409.60411\tvalidation-rmse:545.17511\n","[69]\ttrain-rmse:409.52580\tvalidation-rmse:545.13299\n","[70]\ttrain-rmse:408.72053\tvalidation-rmse:544.77491\n","[71]\ttrain-rmse:407.41467\tvalidation-rmse:544.76693\n","[72]\ttrain-rmse:406.22548\tvalidation-rmse:544.58523\n","[73]\ttrain-rmse:405.13579\tvalidation-rmse:544.58790\n","[74]\ttrain-rmse:403.21449\tvalidation-rmse:544.20643\n","[75]\ttrain-rmse:401.49377\tvalidation-rmse:544.21367\n","[76]\ttrain-rmse:400.26256\tvalidation-rmse:544.37858\n","[77]\ttrain-rmse:399.13349\tvalidation-rmse:544.27528\n","[78]\ttrain-rmse:398.41975\tvalidation-rmse:544.34434\n","[79]\ttrain-rmse:397.02373\tvalidation-rmse:543.90283\n","[80]\ttrain-rmse:395.88816\tvalidation-rmse:544.33808\n","[81]\ttrain-rmse:394.50402\tvalidation-rmse:544.09663\n","[82]\ttrain-rmse:392.17516\tvalidation-rmse:544.86751\n","[83]\ttrain-rmse:390.77213\tvalidation-rmse:544.69666\n","[84]\ttrain-rmse:389.19070\tvalidation-rmse:545.24559\n","[85]\ttrain-rmse:389.13130\tvalidation-rmse:545.07051\n","[86]\ttrain-rmse:387.77570\tvalidation-rmse:545.34295\n","[87]\ttrain-rmse:386.38896\tvalidation-rmse:545.45681\n","[88]\ttrain-rmse:385.79451\tvalidation-rmse:545.25177\n","[89]\ttrain-rmse:385.18080\tvalidation-rmse:545.26247\n","[90]\ttrain-rmse:383.62262\tvalidation-rmse:545.99682\n","[91]\ttrain-rmse:383.32585\tvalidation-rmse:545.84979\n","[92]\ttrain-rmse:382.91168\tvalidation-rmse:545.61669\n","[93]\ttrain-rmse:381.20257\tvalidation-rmse:545.30431\n","[94]\ttrain-rmse:380.88685\tvalidation-rmse:545.46579\n","[95]\ttrain-rmse:380.83559\tvalidation-rmse:545.44038\n","[96]\ttrain-rmse:380.68636\tvalidation-rmse:545.44898\n","[97]\ttrain-rmse:380.48816\tvalidation-rmse:545.30215\n","[98]\ttrain-rmse:379.68519\tvalidation-rmse:545.23124\n","[99]\ttrain-rmse:378.37454\tvalidation-rmse:545.38842\n"]}],"source":["model = xgb.train(\n","   params=params,\n","   dtrain=dtrain_reg,\n","   num_boost_round=n,\n","   evals=evals,\n",")"]},{"cell_type":"markdown","id":"04fe9797-ee79-4aa2-9c78-47e7042d2fc3","metadata":{},"source":["You can see how the model minimizes the score from a whopping ~3931\\$ to just 543\\$.\n","\n","What’s best is that we can see the model’s performance on both our training and validation sets. Usually, the training loss will be lower than validation since the model has already seen the former.\n","\n","In real-world projects, you usually train for thousands of boosting rounds, which means that many rows of output. To reduce them, you can use the `verbose_eval` parameter, which forces XGBoost to print performance updates every `vebose_eval` rounds:"]},{"cell_type":"code","execution_count":51,"id":"2e00b25b-50e5-49a1-8c60-01eb7b6f9eb0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0]\tvalidation-rmse:3930.87087\ttrain-rmse:3985.31595\n","[10]\tvalidation-rmse:591.03042\ttrain-rmse:557.19710\n","[20]\tvalidation-rmse:550.76666\ttrain-rmse:495.31647\n","[30]\tvalidation-rmse:547.16647\ttrain-rmse:467.13670\n","[40]\tvalidation-rmse:544.10422\ttrain-rmse:447.26879\n","[50]\tvalidation-rmse:543.97371\ttrain-rmse:432.51681\n","[60]\tvalidation-rmse:544.77874\ttrain-rmse:420.72943\n","[70]\tvalidation-rmse:544.77491\ttrain-rmse:408.72053\n","[80]\tvalidation-rmse:544.33808\ttrain-rmse:395.88816\n","[90]\tvalidation-rmse:545.99682\ttrain-rmse:383.62262\n","[99]\tvalidation-rmse:545.38842\ttrain-rmse:378.37454\n"]}],"source":["params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\n","n = 100\n","\n","evals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n","\n","\n","model = xgb.train(\n","   params=params,\n","   dtrain=dtrain_reg,\n","   num_boost_round=n,\n","   evals=evals,\n","   verbose_eval=10 # Every ten rounds\n",")"]},{"cell_type":"markdown","id":"d076a028-b0d7-4255-ba93-0476fe83f69a","metadata":{},"source":["## XGBoost Early Stopping\n","\n","By now, you must have realized how important boosting rounds are. Generally, the more rounds there are, the more XGBoost tries to minimize the loss. But this doesn’t mean the loss will always go down. Let’s try with 5000 boosting rounds with the verbosity of 500:"]},{"cell_type":"code","execution_count":52,"id":"3b2f6516-950a-4015-a17f-82d795f48970","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0]\tvalidation-rmse:3930.87087\ttrain-rmse:3985.31595\n","[250]\tvalidation-rmse:557.68923\ttrain-rmse:282.14076\n","[500]\tvalidation-rmse:567.09440\ttrain-rmse:202.29270\n","[750]\tvalidation-rmse:571.09355\ttrain-rmse:153.40105\n","[1000]\tvalidation-rmse:573.92496\ttrain-rmse:124.11251\n","[1250]\tvalidation-rmse:575.80595\ttrain-rmse:102.84756\n","[1500]\tvalidation-rmse:577.05346\ttrain-rmse:86.53817\n","[1750]\tvalidation-rmse:578.72790\ttrain-rmse:74.95409\n","[2000]\tvalidation-rmse:579.29924\ttrain-rmse:64.88654\n","[2250]\tvalidation-rmse:580.10885\ttrain-rmse:56.54860\n","[2500]\tvalidation-rmse:580.64166\ttrain-rmse:49.60797\n","[2750]\tvalidation-rmse:581.06804\ttrain-rmse:43.65126\n","[3000]\tvalidation-rmse:581.33497\ttrain-rmse:39.07501\n","[3250]\tvalidation-rmse:581.68045\ttrain-rmse:35.19566\n","[3500]\tvalidation-rmse:581.94197\ttrain-rmse:31.53707\n","[3750]\tvalidation-rmse:582.01889\ttrain-rmse:28.69983\n","[4000]\tvalidation-rmse:582.09487\ttrain-rmse:26.22932\n","[4250]\tvalidation-rmse:582.27258\ttrain-rmse:24.02182\n","[4500]\tvalidation-rmse:582.35214\ttrain-rmse:22.08176\n","[4750]\tvalidation-rmse:582.38316\ttrain-rmse:20.46374\n","[4999]\tvalidation-rmse:582.41460\ttrain-rmse:19.09406\n"]}],"source":["params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\n","n = 5000\n","\n","evals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n","\n","\n","model = xgb.train(\n","   params=params,\n","   dtrain=dtrain_reg,\n","   num_boost_round=n,\n","   evals=evals,\n","   verbose_eval=250\n",")"]},{"cell_type":"markdown","id":"c977433f-443d-47ce-b5f0-7691e381d8c9","metadata":{},"source":["We get the lowest loss before round 500. After that, even though training loss keeps going down, the validation loss (the one we care about) keeps increasing.\n","\n","When given an unnecessary number of boosting rounds, XGBoost starts to overfit and memorize the dataset. This, in turn, leads to validation performance drop because the model is memorizing instead of generalizing.\n","\n","Remember, we want the **golden middle**: a model that learned just enough patterns in training that it gives the highest performance on the validation set. So, how do we find the perfect number of boosting rounds, then?\n","\n","We will use a technique called **early stopping**. Early stopping forces XGBoost to watch the validation loss, and if it stops improving for a specified number of rounds, it automatically stops training.\n","\n","This means we can set as high a number of boosting rounds as long as we set a sensible number of early stopping rounds.\n","\n","For example, let’s use 10000 boosting rounds and set the `early_stopping_rounds` parameter to 50. This way, XGBoost will automatically stop the training if validation loss doesn't improve for 50 consecutive rounds."]},{"cell_type":"code","execution_count":53,"id":"c1dc4613-d058-43e9-986a-715349e08360","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0]\tvalidation-rmse:3930.87087\ttrain-rmse:3985.31595\n","[100]\tvalidation-rmse:545.56388\ttrain-rmse:377.95218\n","[200]\tvalidation-rmse:554.85898\ttrain-rmse:309.24955\n","[300]\tvalidation-rmse:562.06936\ttrain-rmse:260.56315\n","[400]\tvalidation-rmse:564.23042\ttrain-rmse:228.55880\n","[500]\tvalidation-rmse:567.09440\ttrain-rmse:202.29270\n","[600]\tvalidation-rmse:568.63118\ttrain-rmse:179.81322\n","[700]\tvalidation-rmse:569.82263\ttrain-rmse:162.18310\n","[800]\tvalidation-rmse:571.55832\ttrain-rmse:146.51777\n","[900]\tvalidation-rmse:573.03163\ttrain-rmse:134.76290\n","[1000]\tvalidation-rmse:573.92496\ttrain-rmse:124.11251\n","[1100]\tvalidation-rmse:574.99063\ttrain-rmse:115.12676\n","[1200]\tvalidation-rmse:575.17781\ttrain-rmse:106.35001\n","[1300]\tvalidation-rmse:576.22607\ttrain-rmse:98.92870\n","[1400]\tvalidation-rmse:576.52550\ttrain-rmse:92.87394\n","[1500]\tvalidation-rmse:577.05346\ttrain-rmse:86.53817\n","[1600]\tvalidation-rmse:577.67782\ttrain-rmse:81.69339\n","[1700]\tvalidation-rmse:578.43809\ttrain-rmse:77.47407\n","[1800]\tvalidation-rmse:578.93009\ttrain-rmse:72.85875\n","[1900]\tvalidation-rmse:579.12501\ttrain-rmse:68.27937\n","[2000]\tvalidation-rmse:579.29924\ttrain-rmse:64.88654\n","[2100]\tvalidation-rmse:579.63977\ttrain-rmse:61.38127\n","[2200]\tvalidation-rmse:579.96606\ttrain-rmse:58.11002\n","[2300]\tvalidation-rmse:580.21950\ttrain-rmse:54.76125\n","[2400]\tvalidation-rmse:580.39685\ttrain-rmse:52.30002\n","[2500]\tvalidation-rmse:580.64166\ttrain-rmse:49.60797\n","[2600]\tvalidation-rmse:580.85853\ttrain-rmse:47.04598\n","[2700]\tvalidation-rmse:580.96790\ttrain-rmse:44.71654\n","[2800]\tvalidation-rmse:581.14178\ttrain-rmse:42.70197\n","[2900]\tvalidation-rmse:581.27733\ttrain-rmse:40.85099\n","[3000]\tvalidation-rmse:581.33497\ttrain-rmse:39.07501\n","[3100]\tvalidation-rmse:581.43480\ttrain-rmse:37.39248\n","[3200]\tvalidation-rmse:581.60618\ttrain-rmse:35.76794\n","[3300]\tvalidation-rmse:581.67001\ttrain-rmse:34.41374\n","[3400]\tvalidation-rmse:581.82317\ttrain-rmse:33.05849\n","[3500]\tvalidation-rmse:581.94197\ttrain-rmse:31.53707\n","[3600]\tvalidation-rmse:581.97321\ttrain-rmse:30.27719\n","[3700]\tvalidation-rmse:582.04550\ttrain-rmse:29.21620\n","[3800]\tvalidation-rmse:582.06351\ttrain-rmse:28.03094\n","[3900]\tvalidation-rmse:582.05526\ttrain-rmse:27.09376\n","[4000]\tvalidation-rmse:582.09487\ttrain-rmse:26.22932\n","[4100]\tvalidation-rmse:582.19889\ttrain-rmse:25.32762\n","[4200]\tvalidation-rmse:582.26342\ttrain-rmse:24.39326\n","[4300]\tvalidation-rmse:582.30325\ttrain-rmse:23.55437\n","[4400]\tvalidation-rmse:582.32343\ttrain-rmse:22.80314\n","[4500]\tvalidation-rmse:582.35214\ttrain-rmse:22.08176\n","[4600]\tvalidation-rmse:582.38026\ttrain-rmse:21.40072\n","[4700]\tvalidation-rmse:582.40574\ttrain-rmse:20.71754\n","[4800]\tvalidation-rmse:582.38752\ttrain-rmse:20.16590\n","[4900]\tvalidation-rmse:582.40017\ttrain-rmse:19.62636\n","[4999]\tvalidation-rmse:582.41460\ttrain-rmse:19.09406\n"]}],"source":["n = 5000\n","\n","model = xgb.train(\n","   params=params,\n","   dtrain=dtrain_reg,\n","   num_boost_round=n,\n","   evals=evals,\n","   verbose_eval=100,\n","   # Activate early stopping\n","   early_stopping_rounds=50\n",")"]},{"cell_type":"markdown","id":"560d95bf-c266-498c-9654-bf39aaed8cba","metadata":{},"source":["As you can see, the training stopped after the 167th round because the loss stopped improving for 50 rounds before that.\n","\n","## XGBoost Cross-Validation\n","\n","At the beginning of the tutorial, we set aside 25% of the dataset for testing. The test set would allow us to simulate the conditions of a model in production, where it must generate predictions for unseen data.\n","\n","But only a single test set would not be enough to measure how a model would perform in production accurately. For example, if we perform hyperparameter tuning using only a single training and a single test set, knowledge about the test set would still “leak out.” How?\n","\n","Since we try to find the best value of a hyperparameter by comparing the validation performance of the model on the test set, we will end up with a model that is configured to perform well _only_ on that particular test set. Instead, we want a model that performs well across the board — on any test set we throw at it.\n","\n","A possible workaround is splitting the data into three sets. The model trains on the first set, the second set is used for evaluation and hyperparameter tuning, and the third is the final one we test the model before production.\n","\n","But when data is limited, splitting data into three sets will make the training set sparse, which hurts model performance.\n","\n","The solution to all these problems is cross-validation. In cross-validation, we still have two sets: training and testing.\n","\n","While the test set waits in the corner, we split the training into 3, 5, 7, or _k_ splits or folds. Then, we train the model _k_ times. Each time, we use _k-1_ parts for training and the final _k_th part for validation. This process is called k-fold cross-validation:\n","\n","\n","\n","Source: https://scikit-learn.org/stable/modules/cross_validation.html\n","\n","Above is a visual depiction of a 5-fold cross-validation. After all folds are done, we can take the mean of the scores as the final, most realistic performance of the model.\n","\n","Let’s perform this process in code using the `cv` function of XGB:"]},{"cell_type":"code","execution_count":54,"id":"aa4dd401-a11e-4c8f-84ec-92766b6d0003","metadata":{},"outputs":[],"source":["params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\n","n = 1000\n","\n","results = xgb.cv(\n","   params, dtrain_reg,\n","   num_boost_round=n,\n","   nfold=5,\n","   early_stopping_rounds=20\n",")"]},{"cell_type":"markdown","id":"c2fce8e3-d4fe-4c42-a3cc-e0be225f985c","metadata":{},"source":["The only difference with the train function is adding the `nfold` parameter to specify the number of splits. The results object is now a DataFrame containing each fold's results:"]},{"cell_type":"code","execution_count":55,"id":"4314b768-75af-43cf-ba18-3b68c2bf89cc","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train-rmse-mean</th>\n","      <th>train-rmse-std</th>\n","      <th>test-rmse-mean</th>\n","      <th>test-rmse-std</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3985.648654</td>\n","      <td>10.343596</td>\n","      <td>3986.913623</td>\n","      <td>41.642778</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2848.365726</td>\n","      <td>8.014086</td>\n","      <td>2851.020437</td>\n","      <td>28.028733</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2063.401458</td>\n","      <td>4.637773</td>\n","      <td>2068.629977</td>\n","      <td>19.969459</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1521.493751</td>\n","      <td>3.874078</td>\n","      <td>1530.496272</td>\n","      <td>13.592330</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1156.827103</td>\n","      <td>2.991735</td>\n","      <td>1170.413316</td>\n","      <td>11.695597</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n","0      3985.648654       10.343596     3986.913623      41.642778\n","1      2848.365726        8.014086     2851.020437      28.028733\n","2      2063.401458        4.637773     2068.629977      19.969459\n","3      1521.493751        3.874078     1530.496272      13.592330\n","4      1156.827103        2.991735     1170.413316      11.695597"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["results.head()"]},{"cell_type":"markdown","id":"02d171f0-9d54-4844-b7f8-c5751e23fdc3","metadata":{},"source":["It has the same number of rows as the number of boosting rounds. Each row is the average of all splits for that round. So, to find the best score, we take the minimum of the `test-rmse-mean` column:"]},{"cell_type":"code","execution_count":56,"id":"95805caf-7671-4c4d-8861-20d960602c11","metadata":{},"outputs":[{"data":{"text/plain":["550.7196748119261"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["best_rmse = results['test-rmse-mean'].min()\n","best_rmse"]},{"cell_type":"markdown","id":"e6484ebc-8789-4308-b1fc-be56bcbfa4fd","metadata":{},"source":["Note that this method of cross-validation is used to see the true performance of the model. Once satisfied with its score, you must retrain it on the full data before deployment.\n","\n","## XGBoost Classification\n","\n","Building an XGBoost classifier is as easy as changing the objective function; the rest can stay the same.\n","\n","The two most popular classification objectives are:\n","\n","- `binary:logistic` - binary classification (the target contains only two classes, i.e., cat or dog)\n","- `multi:softprob` - multi-class classification (more than two classes in the target, i.e., apple/orange/banana)\n","\n","\n","Performing binary and multi-class classification in XGBoost is almost identical, so we will go with the latter. Let’s prepare the data for the task first.\n","\n","We want to predict the cut quality of diamonds given their price and their physical measurements. So, we will build the feature/target arrays accordingly:"]},{"cell_type":"code","execution_count":57,"id":"7eaece62-2047-403b-a7ad-e85b23cb4510","metadata":{},"outputs":[],"source":["from sklearn.preprocessing import OrdinalEncoder\n","\n","X, y = diamonds.drop(\"cut\", axis=1), diamonds[['cut']]\n","\n","# Encode y to numeric\n","y_encoded = OrdinalEncoder().fit_transform(y)\n","\n","# Extract text features\n","cats = X.select_dtypes(exclude=np.number).columns.tolist()\n","\n","# Convert to pd.Categorical\n","for col in cats:\n","   X[col] = X[col].astype('category')\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1, stratify=y_encoded)"]},{"cell_type":"markdown","id":"a30bf623-cb0d-4c5f-bde0-dc3063c385c4","metadata":{},"source":["The only difference is that since XGBoost only accepts numbers in the target, we are encoding the text classes in the target with `OrdinalEncoder` of Sklearn.\n","\n","Now, we build the DMatrices…"]},{"cell_type":"code","execution_count":58,"id":"c9befaf0-ec23-4d26-949e-528055ee574b","metadata":{},"outputs":[],"source":["# Create classification matrices\n","dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n","dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)"]},{"cell_type":"markdown","id":"782af288-07eb-44e0-bbc8-dc250be4b8df","metadata":{},"source":["…and set the objective to `multi:softprob`. This objective also requires the number of classes to be set by us:"]},{"cell_type":"code","execution_count":59,"id":"facfe124-1714-40d6-aece-a326b94d9b1d","metadata":{},"outputs":[],"source":["params = {\"objective\": \"multi:softprob\", \"tree_method\": \"hist\", \"num_class\": 5}\n","n = 1000\n","\n","results = xgb.cv(\n","   params, dtrain_clf,\n","   num_boost_round=n,\n","   nfold=5,\n","   metrics=[\"mlogloss\", \"auc\", \"merror\"],\n",")"]},{"cell_type":"markdown","id":"6f7e82bc-2bed-4340-8450-511a1b4e00be","metadata":{},"source":["During cross-validation, we are asking XGBoost to watch three classification metrics which report model performance from three different angles. Here is the result:"]},{"cell_type":"code","execution_count":60,"id":"8d5b6b10-4942-4a23-870f-8cdf9e2e3e54","metadata":{},"outputs":[{"data":{"text/plain":["Index(['train-mlogloss-mean', 'train-mlogloss-std', 'train-auc-mean',\n","       'train-auc-std', 'train-merror-mean', 'train-merror-std',\n","       'test-mlogloss-mean', 'test-mlogloss-std', 'test-auc-mean',\n","       'test-auc-std', 'test-merror-mean', 'test-merror-std'],\n","      dtype='object')"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["results.keys()"]},{"cell_type":"markdown","id":"8cac1535-ea02-436a-ad34-a36e90a167b5","metadata":{},"source":["To see the best AUC score, we take the maximum of test-auc-mean column:"]},{"cell_type":"code","execution_count":61,"id":"42e66f7f-57c3-4500-8cf3-6db55f4c2691","metadata":{},"outputs":[{"data":{"text/plain":["0.9403142623248778"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["results['test-auc-mean'].max()"]},{"cell_type":"markdown","id":"7de5b838-9e3a-48a9-925e-b0ca216daf03","metadata":{},"source":["Even the default configuration gave us 94% performance, which is great.\n","\n","## XGBoost Native vs. XGBoost Sklearn\n","\n","So far, we have been using the native XGBoost API, but its Sklearn API is pretty popular as well.\n","\n","Sklearn is a vast framework with many machine learning algorithms and utilities and has an API syntax loved by almost everyone. Therefore, XGBoost also offers XGBClassifier and XGBRegressor classes so that they can be integrated into the Sklearn ecosystem (at the loss of some of the functionality).\n","\n","If you want to only use the Scikit-learn API whenever possible and only switch to native when you need access to extra functionality, there is a way.\n","\n","After training the XGBoost classifier or regressor, you can convert it using the `get_booster` method:"]},{"cell_type":"code","execution_count":62,"id":"7a0b2588-79b7-44ef-98b4-bbe7e06d6f5b","metadata":{},"outputs":[],"source":["import xgboost as xgb\n","\n","# Train a model using the scikit-learn API\n","xgb_classifier = xgb.XGBClassifier(n_estimators=100, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3, enable_categorical=True)\n","xgb_classifier.fit(X_train, y_train)\n","\n","# Convert the model to a native API model\n","model = xgb_classifier.get_booster()"]},{"cell_type":"markdown","id":"8ec28960-fadc-479f-9bef-b7325ce6bab1","metadata":{},"source":["The model object will behave in the exact same way we've seen throughout this tutorial.\n","\n","## Conclusion\n","\n","We’ve covered a lot of important topics in this XGBoost tutorial, but there are still so many things to learn.\n","\n","You can check out the[ XGBoost parameters page](https://xgboost.readthedocs.io/en/stable/parameter.html), which teaches you how to configure the parameters to squeeze out every last performance from your models.\n","\n","If you are looking for a comprehensive, all-in-one resource to learn the library, check out our [Extreme Gradient Boosting With XGBoost course](https://www.datacamp.com/courses/extreme-gradient-boosting-with-xgboost)."]},{"cell_type":"markdown","id":"1943c7b6","metadata":{},"source":[]},{"cell_type":"markdown","id":"0931a26e","metadata":{},"source":[]},{"cell_type":"markdown","id":"beb10d84","metadata":{},"source":[]}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
